# Innhold
- [Forelesning 2 - Datastrukturer](#forelesning-2---datastrukturer)
    - [Kapittel 10 - ElementÃ¦re datastrukturer](#kapittel-10---elementÃ¦re-datastrukturer)
        - [10.1 Stacks og queues](#101-stacks-og-queues)
        - [10.2 Linked list](#102-linked-list)
    - [Kapittel 11 - Hash tabeller](#kapittel-11---hash-tabeller)
        - [11.1 Direkte-adresse tabeller](#111-direkte-adresse-tabeller)
        - [11.2 Hash tabeller](#112-hash-tabeller)
        - [11.3 Hash funksjoner](#113-hash-funksjoner)
    - [16.4 Dynamic tables from book](#164-dynamic-tables-from-book)
    - [Kapittel 17 - Amortisert analyse](#kapittel-17---amortisert-analyse)
    - [Forelesning 2](#forelesning-2)

# Forelesning 2 - Datastrukturer

- Datastruktur er mÃ¥te Ã¥ lagre og organisere data
    - GjÃ¸r det lettere Ã¥ endre og aksessere dataen
- Ulike datastrukturer har ulike styrker og begrensninger
- Dynamiske sett: sett som kan vokse, krype og endres over tid

- LÃ¦ringsmÃ¥l:
    - ForstÃ¥ hvordan stakker og kÃ¸er fungerer (STACK-EMPTY, PUSH, POP, ENQUEUE, DEQUEUE)
    - ForstÃ¥ hvordan lenkede lister fungerer (LIST-SEARCH, LIST-INSERT, LIST-DELETE)
    - ForstÃ¥ hvordan direkte addressering og hashtabeller fungerer (HASH-INSERT, HASH-SEARCH)
    - ForstÃ¥ konfliktlÃ¸sing ved kjeding (chaining) (CHAINED-HASH-INSERT, CHAINED-HASH-SEARCH, CHAINED-HASH-DELETE)
    - Kjenne til grunnleggende hashfunksjoner
    - Vite at man for statiske datasett kan ha worst-case $O(1)$ for sÃ¸k
    - Kunne definere amorisert analyse
    - ForstÃ¥ aggregert analyse
    - ForstÃ¥ hvordan dynamiske tabeller fungerer (TABLE-INSERT)

## Kapittel 10 - ElementÃ¦re datastrukturer

### 10.1 Stacks og queues
- Stack: Last in, first out (LIFO)
- Queue: First in, first out (FIFO)

#### Stacks
- INSERT: `PUSH` funkjson, setter et element pÃ¥ sluttet
- DELETE: `POP` funksjon, sletter elementet pÃ¥ siste posisjon
- Stack med $n$ elementer er array $S[1,...,n]$
    - `S.top`$ = n$
        - `S.top` $= 0$: $S$ er tom
        - `S.top` $= n$: $S$ er full

##### STACK-EMPTY
- Sjekker om stack er tom
- $O(1)$
```
STACK-EMPTY(S)
1 if S.top == 0
2   return TRUE
3 else return FALSE
```

##### PUSH 
- Legger til et element og Ã¸ker `S.top`
- $O(1)$
```
PUSH(S,x)
1 S.top = S.top + 1
2 S[S.top] = x
```

##### POP
- Fjerner et element
- $O(1)$
```
POP(S)
1 if STACK-EMPTY(S)
2   error "underflow"
3 else S.top = S.top - 1
4   return S[S.top + 1]
```

#### Queues
- Head/hode er front, tail/hale er bak
- Insert: `ENQUEUE` funksjon, legger til element pÃ¥ halen
- Delete: `DEQUEUE` funksjon, fjerner element fra hodet
- Queues er wrap around. Lokasjon 1 fÃ¸lger etter lokasjon $n$ pÃ¥ sirkulÃ¦r mÃ¥te.
- Queue er tom nÃ¥r $Q.tail = Q.head = 1$
- Queue er full nÃ¥r $Q.head=1$ og $Q.tail=Q.length$ eller $Q.head=Q.tail+1$ 
    - Full nÃ¥r der kun er en ledig plass igjen i en array, sÃ¥ man kan skille mellom head og tail

##### ENQUEUE
- Legger til element $x$ ved $Q.tail$. Om $Q.tail$ er enden av array flyttes halen til started (wraparound). Ellers Ã¸ker $Q.tail$Â med en. 
- FÃ¥r overflow om man kjÃ¸rer `ENQUEUE` pÃ¥ full lisre
```
ENQUEUE(Q,x):
1 Q[Q.tail] = x
2 if Q.tail == Q.length
3   Q.tail = 1
4 else Q.tail = Q.tail + 1
```
- KjÃ¸retid: $O(1)$

##### DEQUEUE
- Fjerner hodet, lagrer det i $x$ og returnerer $x$
- Om $Q.head$ er i enden av arrayen flytted $Q.head$Â til starten, ellers Ã¸ker man det med 1
```
DEQUEUE(Q):
1 x = Q[Q.head]
2 if Q.head == Q.length
3   Q.head = 1
4 else Q.head = Q.head + 1
5 return x
```
- KjÃ¸retid: $O(1)$

### 10.2 Linked list
- Datastruktur med objekter linjet lineÃ¦rt
- RekkefÃ¸lge bestemmes av peker til hvert objekt
- Dobbelt linked list: hvert element er objekt med en $key$ egenskap og to pekeregenskaper, $prev$ og $next$
    - Ved element $x$ vil $x.next$ peke mot neste element og $x.prev$Â pÃ¥ forrige element.
    - $x.next=NIL$ betyr at det ikke er noe neste element, $x$ er halen
    - $x.prev=NIL$ betyr at det ikke er noe forrige element, $x$ er hodet
- Enkelt linked list: som dobbelt linked list, uten $prev$ peker
<img src="assets/assets30/linkedLists.png" height="200px">
- Linked list kan vÃ¦re sortert, sÃ¥ lineÃ¦r rekkefÃ¸lge til listen er lik lineÃ¦r rekkefÃ¸lge til $key$-verdiene til elementene (minste elementet ved hodet, stÃ¸rste ved halen)
- Linked list kan vÃ¦re sirkulÃ¦r

#### LIST-SEARCH
- Finner fÃ¸rste element med $key$ $k$ i liste $L$. Returnerer peker til elementet (NIL om objectet ikke finnes)
- Worstcase kjÃ¸retid: $\Theta(n)$
```
LIST-SEARCH(L,k)
1 x = L.head
2 while x â‰  NIL and x.key â‰  k
3   x = x.next
4 return x
```

#### LIST-INSERT
- Legger til element i fronten av lista
- $x$ fÃ¥r $next$ peker til fÃ¸rste element i $L$. Om $L$ har et fÃ¸rste element setter vi $prev$ pekeren til dette objektet til $x$. Om $L$ er tom vil $x.next$ vÃ¦re $NIL$. Setter fÃ¸rste element i $L$Â lik $x$ og $prev$ pekeren til $x$ er derfor $NIL$.
- KjÃ¸retid: $O(1)$  
```
LIST-INSERT(L,x)
1 x.next = L.head
2 if L.head â‰  NIL
3   L.head.prev = x
4 L.head = x
5 x.prev = NIL
```

#### LIST-DELETE
- Fjerner elementet $x$ 
- Om $x$ har et element foran mÃ¥ $next$ pekeren settes lik $x$ sin $next$ peker. kan vÃ¦re et annet element eller $NIL$
- Om $x$ er fÃ¸rste element i listen 
- KjÃ¸retid: $O(1)$ tid for Ã¥ fjerne element, med mÃ¥ bruke `LIST-SEARCH` for Ã¥ hente peker til $x$ i elementet. Derfor kjÃ¸retid $\Theta(n)$
    - Dobbel lenket liste har allerede pekere i begge retninger, sÃ¥ dette tar $O(1)$

## Kapittel 11 - Hash tabeller
- StÃ¸tter dictionary operasjoner som `INSERT`, `SEARCH` og `DELETE`
- Hashing er effektivt
    - Average kjÃ¸retid i hash tabell: $O(1)$
- Direkte adressering av array undersÃ¸ker en vilkÃ¥rlig posisjon ilÃ¸pet av tiden $O(1)$
    - Bruker direkte adressering med array med posisjon for alle keys
    - NÃ¥r antall keys er lite relativt til antall mulige keys vil hash tabeller bli effektivt alternativ til direkte adressering, da hash tabeller ofte bruker array med stÃ¸rrelse proporsjonal til antall keys som er lagret

### 11.1 Direkte-adresse tabeller
- Direkte adressering er en enkel teknikk, fungerer bra nÃ¥r universet $U$ av $keys$ er relativt lite.
- Anta dynamisk sett, hvert element har nÃ¸kker fra $U=\{0,1,...,m-1\}$ der $m$ ikke er stor og ingen elementer har samme key
    - Bruker array for Ã¥ representere settet, kalles direkte-adresse tabell $T[0,...,m-1]$ der hver $key$ i universet korresponderer til index i tabellen. 

<img src="assets/assets30/directAddressing.png" height="300px">

#### DIRECT-ADDRESS-SEARCH
- Returnerer elementet i direkte-adress tabellen med index $k$ (eller NIL). ELementet peker mot elementet i settet med $key$ $k$.

```
DIRECT-ADDRESS-SEARCH(T,k)
1 return T[k]
```

#### DIRECT-ADDRESS-INSERT
- Setter $x$ i direkte-adresse tabell ved index gitt av $key$-verdien til elementet

```
DIRECT-ADDRESS-INSERT(T,x)
1 T[x.key] = x
```

#### DIRECT-ADDRESS-DELETE
- Fjerner elementet $xÂ£ fra direkte-adresse tabellen ved index gitt av $key$ verdi til elementet ved Ã¥ sette elementet ved denne indeksen lik NIL

```
DIRECT-ADDRESS-DELETE(T,x)
1 T[x.key] = NIL
```

### 11.2 Hash tabeller
- Om $U$Â er stort kan det vÃ¦re upraktisk Ã¥ lagre tabellen $T$ av stÃ¸rrelse $|U|$. Mye av rommet kan ogsÃ¥ vÃ¦re bortkastet, om antall $keys$ er mye mindre enne $U$
- Kan redusere romkrav til $|\Theta(K)|$ ved Ã¥ bruke hashtabeller, Ã¥ fortsatt ha $O(1)$ tid.
- I direkte adressering er elementet med $key$ $k$ lagret i luke $k$, men i hashing er elementet lagret i luke $h(k)$.
    - Hashing bruker hash funksjon $h$ pÃ¥ $key k$ for Ã¥ regne ut indexen til luken.
    - Hash funksjon tar $key$ fra $U$ og regner ut indeksen til luke i hash tabell $T[0,1,...,m-1]$: $h: U\to {0,1,...,m-1}$
    - StÃ¸rrelsen $m$ til hashtabellen er ofte mye mindre enn $|U|$
    - Elementet med key $k$ hasher til luke $h(k)$
    - $h(k)$ er hash verdien til key $k$

<img src="assets/assets30/hashingFunctions.png" height="300px">

#### HASH-INSERT

- MÃ¥l: Sette inn nÃ¸kkelen k i hash-tabellen ğ‘‡.
- Initialiserer probe-verdien ğ‘– til 0.
- Beregner hash-verdien for nÃ¸kkelen k og probe-verdien ğ‘–, kalt ğ‘—.
- SÃ¸keprosess:
    - Sjekker luken ved posisjon ğ‘— i tabellen ğ‘‡.
    - Hvis luken er tom, plasseres k i denne luken.
    - Hvis luken er opptatt, Ã¸kes probe-verdien ğ‘– med Ã©n.
- Iterasjon:
    - Fortsetter Ã¥ Ã¸ke ğ‘– og sjekke etterfÃ¸lgende luker.
    - Stopper nÃ¥r en tom luke er funnet eller probe-verdien ğ‘– nÃ¥r ğ‘š.
- Returverdier:
    - Returnerer indeksen til luken hvor k er plassert.
    - Returnerer feilmelding om overflow hvis hele tabellen er gjennomgÃ¥tt uten Ã¥ finne en tom luke.

```
HASH-INSERT(T,k)
1 i = 0
2 repeat
3   j = h(k,i)
4   if T[j] == NIL
5       T[j] = k
6       return j
7   else i = i + 1
8 until i == m 
9 error "hash table overflow"
```

#### HASH-SEARCH
- For Ã¥ bestemme hva som skal undersÃ¸kes inkluderer hashfunksjonen et probenummer $i$ som input, sÃ¥ $h:U\times \{0,1,...,m-1\}\to \{0,1,...,m-1\}$
- Initialiserer probe-verdien ğ‘– til 0.
- Setter ğ‘— lik hash-verdien for nÃ¸kkelen k og probe-verdien ğ‘–.
- Sjekker om luken ved posisjon ğ‘— inneholder k.
    - Hvis ja, returnerer posisjon ğ‘—.
    - Hvis nei, Ã¸ker ğ‘– med Ã©n og gjentar prosessen.
- Fortsetter til en av fÃ¸lgende inntreffer:
    - En luke som inneholder k er funnet.
    - En tom luke (ğ‘ğ¼ğ¿) er funnet, indikerer at k ikke er i tabellen.
    - Probe-verdien ğ‘– nÃ¥r ğ‘š, hele tabellen er sjekket.
- Returnerer posisjon ğ‘— hvis k er funnet, ellers NIL.

```
HASH-SEARCH(T,k)
1 i = 0
2 repeat
3   j = h(k,i)
4   if T[j] == k 
5       return j
6   i = i + 1
7 until T[j] == NIL or i == m
8 return NIL
```

#### DELETE i Hash-Tabell

- Kan ikke enkelt fjerne en nÃ¸kkel ved Ã¥ sette luken til NIL.
- Dette kan forstyrre gjenfinning av andre nÃ¸kler.
- LÃ¸sning: Marker luker som DELETED istedenfor NIL.
    - Tilpasning av HASH-INSERT: Tillater innsetting i luker merket som DELETED. HASH-SEARCH opererer som normalt, ignorerer DELETED-merkede luker.

#### Kollisjon
- To keys kan hashe til samme luke, kalles kollisjon 
- Kan minimere kollisjoner ved at hashfunksjonen fremstÃ¥r tilfeldig (men $h$ mÃ¥ vÃ¦re deterministisk, input $k$ skal alltid gi outbut $h(k))
- Siden $|U|>m$ mÃ¥ det vÃ¦re minst to keys med samme hashverdi, sÃ¥ kan ikke helt unngÃ¥ kollisjoner

#### Kollisjon hÃ¥ndtering ved chaining
- LÃ¸sning pÃ¥ kollisjon
- Elementer som hasher til samme luke plasered i linked list. Luke $j$ har peker som peker mot hodet til listen av elementer som hasher til $j$

##### CHAINED-HASH-INSERT
- Setter element $x$ ved hodet i linked list (ligger ved luken gitt av posisjonen $h(x.key)$)
- Worstcase kjÃ¸retid: $O(1)$, rask fordi den antar $x$ ikke er i tabellen allerede

```
CHAINED-HASH-INSERT(T,x)
1 insert x at the head of list T[h(x.key)]
```

##### CHAINED-HASH-SEARCH
- Luken ved posisjon $h(k)$ peker mot linked list og metoden undersÃ¸ker om den inneholder elementet med $k$
- Worstcase kjÃ¸retid: $\Theta(n)$
```
CHAINED-HASH-SEARCH(T,k)
1 search for an element with key k in list T[h(k)]
```
##### CHAINED-HASH-DELETE
- Metoden finner luken med posisjon $h(x.key)$ og fjerner $x$ ved Ã¥ oppdatere pekerne til elementet foran og etter $x$. 
- KjÃ¸retid ved dobbel linked list: $O(1)$
- KjÃ¸retid ved single linked list: $O(n)$

```
CHAINED-HASH-DELETE(T.x)
1 delete x from the list T[h(x.key)]
```

#### Analyse av hashing med chaining
- Lastfaktor (Load Factor) ğœ¶: Definert som ğœ¶ = ğ‘›/ğ‘š, hvor ğ‘› er antall elementer og ğ‘š er antall luker i hash-tabellen.
- Worst-Case Scenario:
    - Alle ğ‘› keys hasher til samme luke, skaper en liste med lengde ğ‘›.
    - Worst-case kjÃ¸retid: ğ›©(ğ‘›) pluss tiden for Ã¥ beregne hash-funksjonen.
    - Hash-tabeller brukes ikke for worst-case scenarier.
- Average-Case KjÃ¸retid:
    - Avhenger av hvor godt hash-funksjonen distribuerer keys blant de ğ‘š lukene.
    - Antagelse: Simple Uniform Hashing â€“ sannsynligheten for at et element hasher til en luke er lik for alle luker.
    - Forventet lengde pÃ¥ listen ğ‘‡[ğ‘—] for ğ‘— = 0, 1, â€¦ , ğ‘š âˆ’ 1 er ğ›¼ = ğ‘›/ğ‘š.
- KjÃ¸retid for SÃ¸k:
    - Avhenger av lengden til listen i luken ğ‘‡[â„(ğ‘˜)].
    - Beregning av hash-verdien â„(ğ‘˜) tar ğœª(1) tid.
- To SÃ¸ketilfeller:
    1. Ikke-suksessfullt SÃ¸k:
        - Gjennomsnittlig kjÃ¸retid: ğœ£(1 + ğœ¶).
            - Tid for Ã¥ finne hashfunksjon + forventet tid for Ã¥ sÃ¸ke til enden av listen med lengde ğ›¼.
        - Total forventet kjÃ¸retid: ğ›©(1 + ğ›¼).
    2. Suksessfullt SÃ¸k:
        - Gjennomsnittlig kjÃ¸retid: ğœ£(1 + ğœ¶/2).
        - Antagelse: LineÃ¦rt sÃ¸k i en ikke-sortert liste sÃ¸ker gjennomsnittlig gjennom halvparten av elementene.
        - Total forventet kjÃ¸retid: ğ›©(1 + ğ›¼/2).
- Average KjÃ¸retid for SÃ¸k:
    - Hvis ğ‘› = ğ›°(ğ‘š), blir gjennomsnittlig kjÃ¸retid for sÃ¸k ğ‘¶(1).
    - INSERTION og DELETION har ogsÃ¥ ğ‘¶(1) tid worst-case.
    - Alle ordbok operasjonene i hashing tar ğœª(1) tid average-case.

### 11.3 Hash funksjoner
#### Hva utgjÃ¸r en god hashfunksjon?
- Egenskaper for en God Hash-funksjon:
    - NÃ¦rmer seg Simple Uniform Hashing: Hver nÃ¸kkel like sannsynlig Ã¥ hashe til enhver av de ğ‘š lukene.
    - Uavhengighet: Hashing av en nÃ¸kkel er uavhengig av hvor andre nÃ¸kler har hashet.
    - Vanskelig Ã¥ UndersÃ¸ke: Sannsynlighetsfordelingen ofte ukjent, og nÃ¸kler er ikke alltid uavhengige.
    - Kjent Distribusjon Scenario: NÃ¥r nÃ¸kler er tilfeldige, reelle tall mellom 0 og 1, uniformt distribuert, kan en hashfunksjon som â„(ğ‘˜) = âŒŠğ‘˜ğ‘šâŒ‹ oppnÃ¥ simpel uniform hashing.
- Minimering av Kollisjoner: En god hash-funksjon reduserer sannsynligheten for at lignende nÃ¸kler hasher til samme luke.
- Uavhengig av Data MÃ¸nstre: Hashing-effektiviteten bÃ¸r ikke pÃ¥virkes av mÃ¸nstre i dataene.
- Separasjon av 'NÃ¦re' NÃ¸kler: Ã˜nskelig at nÃ¸kler som er nÃ¦r hverandre fÃ¥r svÃ¦rt ulike hash-verdier.
- Matematisk og Deterministisk Natur: Hash-funksjonen mÃ¥ gi samme resultat ved gjentatte beregninger av â„(ğ‘˜) for samme nÃ¸kkel.

#### Tolkning av keys som naturlige tall
- Univers av NÃ¸kler: De fleste hash-funksjoner forutsetter at universet av nÃ¸kler er naturlige tall (ğ‘ = {0, 1, 2, â€¦}).
- Tilpasning for Ikke-Naturlige NÃ¸kler: NÃ¥r nÃ¸kler ikke er naturlige tall, mÃ¥ de tolkes eller konverteres til det.
- Eksempel med Character Strings: Character strings kan omformes til heltall ved hjelp av ASCII-tabellen.
    - For eksempel, strengen "ğ‘ğ‘¡" konverteres til (112, 116), hvor ğ‘ er 112 og ğ‘¡ er 116 i ASCII.

#### Divisjonsmetoden
- Kartlegger en nÃ¸kkel ğ‘˜ til en av ğ‘š lukene ved Ã¥ bruke rest-operasjonen: â„(ğ‘˜) = ğ‘˜ mod ğ‘š.
- For eksempel, med ğ‘š = 12 og ğ‘˜ = 100, blir â„(ğ‘˜) = 4.
- Fordel: Metoden er rask pÃ¥ grunn av en enkel divisjonsoperasjon.
- Valg av ğ‘š:
    - Ikke bruk potens av 2 (f.eks., ğ‘š = 2^6), da dette begrenser â„(ğ‘˜) til de laveste-orden bits av ğ‘˜, som kanskje ikke er jevnt fordelt.
- Ideelt Ã¥ velge ğ‘š som et primtall, helst ikke nÃ¦r en potens av to.
- Eksempel med Primtall:
    - For en hash-tabell som skal inneholde ğ‘› = 2000 character strings og kan tolerere 3 elementer per luke ved ikke-suksessfullt sÃ¸k (2000/3), velg ğ‘š = 701.
    - 701 er et primtall nÃ¦r 2000/3 og ikke nÃ¦r en toerpotens.
    - I dette tilfellet blir â„(ğ‘˜) = 2000 mod 701.

#### Multiplikasjonsmetoden
- Opererer i to trinn:
    1. Henter ut fraksjonsdelen av ğ‘˜ğ´, hvor 0 < ğ´ < 1.
    2. Multipliserer denne fraksjonsdelen med ğ‘š og tar gulvverdien av resultatet.
    - Hash-funksjonen: â„(ğ‘˜) = âŒŠğ‘š(ğ‘˜ğ´ mod 1)âŒ‹.
    - Fraksjonsdelen beregnes som ğ‘˜ğ´ mod 1 = ğ‘˜ğ´ âˆ’ âŒŠğ‘˜ğ´âŒ‹.
- Fordeler med Multiplikasjonsmetoden:
    - Verdien av ğ‘š er ikke kritisk.
    - Ofte velges ğ‘š som en potens av to, noe som gjÃ¸r metoden enkel Ã¥ implementere pÃ¥ datamaskiner.

$$
c_i = 
\begin{cases} 
i & \text{if } i - 1 \text{ is an exact power of 2},\\
1 & \text{otherwise}.
\end{cases}
$$

## 16.4 Dynamic tables from book
- TABLE-INSERT is used to add items to a dynamically sized table.
- If the table is empty, it is initialized with one slot.
- When inserting, if the table is full (number of items equals table size), the table size is doubled.
- All items from the old table are copied into the new, larger table.
- The cost of TABLE-INSERT is analyzed by assigning a cost of 1 to each elementary insertion.
- The actual running time of TABLE-INSERT is mostly affected by the cost of copying items during table expansion.
- Table expansion occurs when the current table is full, leading to a cost of i for the ith insertion, accounting for the new insertion plus copies of old items.
- The worst-case cost per operation is $O(n)$, leading to $O(n^2)$ over $n$ operations, but this is not tight due to infrequent expansions.
- The total cost for $n$ TABLE-INSERT operations is less than $3n$, setting the amortized cost per operation at $O(1)$.
- Dynamic tables resize themselves based on the number of items, supporting insertion and deletion without knowing the maximum number of items in advance.
- The load factor is the ratio of items to slots in the table, aiming to keep unused space below a constant fraction of the total space.
- The actual cost of TABLE-INSERT varies: 1 if there's room, and i (the number of items) when expansion is needed.
- Amortized analysis using the accounting method shows that the cost of TABLE-INSERT over many operations averages to O(1), even with the occasional expensive expansion operation.
- The figure demonstrates the accounting method for the cost analysis of TABLE-INSERT operations, with each operation charging $3 to account for current insertion, future reinsertion, and the reinsertion of existing items during table expansion.

## Kapittel 17 - Amortisert analyse
- Algoritme kan vÃ¦re rask majoriteten av tiden, men noen ganger veldig treg
- Amortisert analyse tar gjennomsnittet at tid for Ã¥ utfÃ¸re sekvens av datastruktur operasjoner over alle operasjonene som utfÃ¸res
    - Gjennomsnitt kjÃ¸retid per operasjon etter mange har bltt utfÃ¸rt, istedetfor kjÃ¸retid for enkeltoperasjon
    - Viser at gjennomsnittlig kostnad for en operasjon kan vÃ¦re liten, selv om en enkelt operasjon kan vÃ¦re dyr, ved Ã¥ ta gjennomsnittlig kostnad over en sekvens operasjoner.
- Sannsynlighet er ikke involvert i amortisert analyse
- Garanterer gjennomsnittlig ytelse for hver operasjon i worstcase
- Gir innsikt om bestemt datastruktur, som kan gjelpe optimalisere designet. Vil ofte vÃ¦re bedre enn worst-case analyse, fordi worst-case kan vÃ¦re pessimistisk.

- Aggregat analyse: bestemmer Ã¸vre grense $T(n)$ pÃ¥ totale kostnaden for sekvens av $n$ operasjoner. Gjennomsnittlig kostnad (amortisert kostnad) per operasjon blir $\frac{T(n)}{n}$

## Forelesning 2
### Dynamiske tabeller 
- NÃ¥r man setter element i hashtabell, stakk eller kÃ¸ som er full mÃ¥ man allokere nytt minne og kopiere inn elementene. Tar lineÃ¦r tid, sÃ¥ gjÃ¸r sjeldent.,

